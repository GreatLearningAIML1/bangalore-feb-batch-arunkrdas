{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab_R7_Internal_FMNIST_CNN_CIFAR_DATA_Augment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyfMmMnPJjvn"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjcGOJhcJjvp"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jR0Pl2XjJjvq"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qr75v_UYJjvs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63d0a5ea-124a-4e58-d105-69e3b98a685c"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTI42-0qJjvw"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiJY22cgOCe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2sf67VoJjvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4664c965-625a-4f41-dc69-c95217f2bfde"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zewyDcBlJjv1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c635e454-f0fa-4254-f64c-389863094899"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WytT2eRnJjv4"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYkT2JsdOCfM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff9be4d9-78df-425c-ffc9-5a3f88d7806f"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XycQGBSGJjv5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "58375c8b-19c6-4533-8494-e3179fd542a1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "image = x_train[0]\n",
        "plt.figure(0)\n",
        "plt.imshow(image)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f33f6fc9a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5jtdZ7RqJjv8"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sAD3q5I6Jjv9",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train,num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgHSCXy3JjwA",
        "colab": {}
      },
      "source": [
        "y_test = keras.utils.to_categorical(y_test,num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xO5BRBzBJjwD"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3fUQpMHxJjwE",
        "colab": {}
      },
      "source": [
        "x_train = x_train/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Okwo_SB5JjwI",
        "colab": {}
      },
      "source": [
        "x_test = x_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "da5-DwgrJjwM"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LPGVQ-JJJjwN",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZj9AdDdOCgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFRRTJq8JjwQ"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dWTZYnKSJjwR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C18AoS7eJjwU"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3x3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3bB2KrCOChS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# early stopping patience=5 -- if accuracy is not improving after 5 epochs then do not proceed further\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DORCLgSwJjwV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "71f2e443-1f21-4a59-fe53-4c455bef1f9c"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu')) \n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu')) \n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(128, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=100, batch_size=32, callbacks=[es])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0818 11:40:46.151682 139862415411072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0818 11:40:46.167415 139862415411072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0818 11:40:46.183250 139862415411072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0818 11:40:46.185250 139862415411072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0818 11:40:46.242265 139862415411072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0818 11:40:46.327717 139862415411072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0818 11:40:46.471058 139862415411072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 13s 223us/step - loss: 0.3742 - acc: 0.8651 - val_loss: 0.3011 - val_acc: 0.8876\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.2353 - acc: 0.9132 - val_loss: 0.2529 - val_acc: 0.9055\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.1711 - acc: 0.9365 - val_loss: 0.2618 - val_acc: 0.9097\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.1198 - acc: 0.9555 - val_loss: 0.2585 - val_acc: 0.9143\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0807 - acc: 0.9706 - val_loss: 0.3064 - val_acc: 0.9111\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0539 - acc: 0.9810 - val_loss: 0.3613 - val_acc: 0.9099\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.0396 - acc: 0.9861 - val_loss: 0.3923 - val_acc: 0.9136\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0270 - acc: 0.9904 - val_loss: 0.4351 - val_acc: 0.9098\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0256 - acc: 0.9913 - val_loss: 0.4352 - val_acc: 0.9139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33f3c27a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ju69vKdIJjwX"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L2hAP94vJjwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "1105c439-ad7d-494b-fef4-8b6877ae0231"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu')) \n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(128, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=100, batch_size=32, callbacks=[es])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0818 11:42:24.180019 139862415411072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0818 11:42:24.195096 139862415411072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.3865 - acc: 0.8616 - val_loss: 0.3089 - val_acc: 0.8869\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.2484 - acc: 0.9098 - val_loss: 0.2469 - val_acc: 0.9121\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.2014 - acc: 0.9257 - val_loss: 0.2286 - val_acc: 0.9184\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.1655 - acc: 0.9382 - val_loss: 0.2208 - val_acc: 0.9218\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.1362 - acc: 0.9485 - val_loss: 0.2407 - val_acc: 0.9138\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.1140 - acc: 0.9568 - val_loss: 0.2350 - val_acc: 0.9216\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0951 - acc: 0.9643 - val_loss: 0.2601 - val_acc: 0.9208\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0785 - acc: 0.9706 - val_loss: 0.2569 - val_acc: 0.9208\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0684 - acc: 0.9748 - val_loss: 0.2949 - val_acc: 0.9240\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0584 - acc: 0.9783 - val_loss: 0.2866 - val_acc: 0.9232\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0512 - acc: 0.9807 - val_loss: 0.3244 - val_acc: 0.9197\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0471 - acc: 0.9827 - val_loss: 0.3006 - val_acc: 0.9270\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0409 - acc: 0.9849 - val_loss: 0.3365 - val_acc: 0.9189\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0384 - acc: 0.9856 - val_loss: 0.3597 - val_acc: 0.9246\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0352 - acc: 0.9872 - val_loss: 0.3489 - val_acc: 0.9221\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0334 - acc: 0.9876 - val_loss: 0.3897 - val_acc: 0.9242\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0289 - acc: 0.9898 - val_loss: 0.3923 - val_acc: 0.9231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33e633a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lGTA3bfEJjwa"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F6gX8n5SJjwb"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cbz4uHBuJjwc",
        "colab": {}
      },
      "source": [
        "img_generator = keras.preprocessing.image.ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
        "img_generator.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl-8dOo7Jjwf"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DpI1_McYJjwg",
        "outputId": "93176d48-bd9e-4dd1-c0f4-080e2059bc27",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "gen = img_generator.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmPl5yE8Jjwm"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44ZnDdJYJjwn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "04c1fa10-5028-4f64-a2fb-8149b1db300d"
      },
      "source": [
        "batch_size=64\n",
        "model.fit_generator(img_generator.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_test, y_test), steps_per_epoch=len(x_train)//batch_size, epochs=10, callbacks=[es])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.6691 - acc: 0.7584 - val_loss: 0.4341 - val_acc: 0.8453\n",
            "Epoch 2/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.5097 - acc: 0.8108 - val_loss: 0.3983 - val_acc: 0.8537\n",
            "Epoch 3/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.4747 - acc: 0.8240 - val_loss: 0.3707 - val_acc: 0.8648\n",
            "Epoch 4/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.4537 - acc: 0.8328 - val_loss: 0.3456 - val_acc: 0.8718\n",
            "Epoch 5/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.4391 - acc: 0.8392 - val_loss: 0.3525 - val_acc: 0.8713\n",
            "Epoch 6/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.4278 - acc: 0.8406 - val_loss: 0.3558 - val_acc: 0.8662\n",
            "Epoch 7/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.4184 - acc: 0.8457 - val_loss: 0.3297 - val_acc: 0.8769\n",
            "Epoch 8/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.4084 - acc: 0.8499 - val_loss: 0.3433 - val_acc: 0.8729\n",
            "Epoch 9/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.3993 - acc: 0.8532 - val_loss: 0.3383 - val_acc: 0.8733\n",
            "Epoch 10/10\n",
            "937/937 [==============================] - 15s 16ms/step - loss: 0.3933 - acc: 0.8542 - val_loss: 0.3372 - val_acc: 0.8755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33e029f438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MwQQW5iOJjwq"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c1SrtBEPJjwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0e1b3056-a6d1-4eb9-aa9a-dd6fcf0f7d8b"
      },
      "source": [
        "model.evaluate(x_train, y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 3s 56us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3042815688967705, 0.8853333333333333]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBwVWNQC2qZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8115f0af-583c-4f47-85b7-0239ecca015a"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 55us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3371911446809769, 0.8755]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8KXqmUDW2rM1"
      },
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8mja6OgQ3L18"
      },
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6HzVTPUM3WZJ"
      },
      "source": [
        "### **Import neessary libraries for data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PPM558TX4KMb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W6hicLwP4SqY"
      },
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NQ1WzrXd4WNk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7055cc15-bb91-4143-bfe5-c062762d1b54"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqPftrSBnCcc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ead25a83-858f-4626-d3ae-9df44227d790"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1jVtuTmnG3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc9dfd71-b9ec-4af9-8563-58c49398d5dd"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCYlIy7TnV4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo_b_BP2nkxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JN3vYYhK4W0u"
      },
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JJbekTKi4cmM",
        "colab": {}
      },
      "source": [
        "data_gen = keras.preprocessing.image.ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e-SLtUhC4dK2"
      },
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CSw8Bv2_4hb0",
        "colab": {}
      },
      "source": [
        "data_gen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gYyF-P8O4jQ8"
      },
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mXug4z234mwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "a07d490f-cae2-43a0-91d3-bb2be5ac0d7a"
      },
      "source": [
        "gen = data_gen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABcCAYAAAB3AO7GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAYJJREFUeJzt2zEKw0AMAMEo5P9fVh5gF8FFzOKZ\n+uCEiuWam919AdDzvnsAAK4RcIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeI+vzzspl5xLfP3Z1f\nz9rJkZ2cs5ejp+/ECxwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwg\nSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBK\nwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErA\nAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIGp29+4ZALjACxwgSsABogQc\nIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwg\nSsABogQcIErAAaIEHCDqC449GrOJLA48AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKuRYOQlm-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}