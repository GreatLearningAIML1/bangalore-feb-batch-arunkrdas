{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYk8NG3yOIT9"
   },
   "source": [
    "### A MNIST-like fashion product database\n",
    "\n",
    "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFO6PuxzOIT_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efNjNImfOIUC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9C4aAIGOIUH",
    "outputId": "5ef9aff6-a7bd-4b26-cba6-8750955f6ca3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcoZBStrOIUQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA1WsFSeOIUS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnbx7TyQOIUY"
   },
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xtest, ytest) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbiHj5YPOIUc",
    "outputId": "87e1b9cd-07f0-45cb-e706-0d51ad742d72",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape: (60000, 28, 28) ytrain shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"xtrain shape:\", xtrain.shape, \"ytrain shape:\", ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtest shape: (10000, 28, 28) ytest shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"xtest shape:\", xtest.shape, \"ytest shape:\", ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 ... 3 0 5]\n"
     ]
    }
   ],
   "source": [
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDAYzkwyOIUj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convert both training and testing labels into one-hot vectors.\n",
    "\n",
    "**Hint:** check **tf.keras.utils.to_categorical()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBlfYlANOIUk"
   },
   "outputs": [],
   "source": [
    "from keras.utils import normalize, to_categorical\n",
    "\n",
    "trainY = keras.utils.to_categorical(ytrain)\n",
    "testY = keras.utils.to_categorical(ytest)\n",
    "\n",
    "trainX = normalize(xtrain, axis=1)\n",
    "testX = normalize(xtest, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHV3b9mzOIUq",
    "outputId": "27bdfe58-91ee-4677-fe49-e742ad306c70",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "test data after encoded: [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainY.shape)\n",
    "print('test data after encoded:', trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhQ8e7VOIUw"
   },
   "source": [
    "### Visualize the data\n",
    "\n",
    "Plot first 10 images in the triaining set and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvDML2OoOIUx",
    "outputId": "9dafc94e-61a8-4089-be03-d143163d68aa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAADuCAYAAADRE7iBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztvXmcJFd153tORmbWXtVd1bvUi1pba6UltSQkdmEQAhtkI4zZbIH97MEee3jzmPdsY2PDzBhjewxjsM2A2Wxj4IMA8wEbBMiITUILqLW0dqnV6lbv1bUvuUTc90dmxznnVkZ0VnUtWRW/7+fTn76R98aNm3HjRt46KzvnCAAAAAAgS+SWegAAAAAAAIsNNkAAAAAAyBzYAAEAAAAgc2ADBAAAAIDMgQ0QAAAAADIHNkAAAAAAyBzYAAEAAAAgc2ADBAAAAIDMgQ0QAAAAADIHNkAAAAAAyBz52TQucptrp66FGgtowDRNUNmVeL77bZW55HwQl6POtricK4e2YVUfp6RvCaQ/V1SPd+idMzWtulu8dDBjNHTcObd2vvtdqvnkQsEclweKcbk4WI7LrlI5/Yt1dcTFsMP+7RYMTsrBIs3nSlibnFdrpGB/DsIOWUsUSTGYrtpOSjLPuo+ws2iaRap7VlMUTEemHU1MpY55oViItdkq79mFhHOyFl0UpbRcPJqdy1ltgNqpi67ml899VGDW3OVuW5B+W2Uug1X9cXnqyrPjcsezI7bh4LCUQ7UZ8n/sVvfFxfLm1XE5P+79AD/wuHRRKs1ixKfHd90t+xai36Waz/yGM8zxM2/bGpe3/ZN81eqB507/YhdfEhdPXNxtqgb+6adx2VXKtBishLUZ9MtvhDvD/l6M7OiVdmVZZz2PDpl27uln43Ju7Zq4PLrLPhtTA/JDGagp6nty0rTjO+5vZujzzkKszVZ5zy4kuU7Z4EUTE0s4EqHZuZzVBghkEFZ/4Db5l3V+62ZzHK5bFZcnNneauiighpQu7TfH1fYBGZLe/3hKXFZ/gBQn5KC02koqpi69PC53DEk7jux37PnpQRnD/gONB5sxeNfFcfm5a3tN3UWveSwuH7tONin7Dl1u2rlJefVwRZ4x122lC21d8kt51WZ5pz1x94V2TNEVcXn1Z+5M/wIZo3qd3JvBi9pMXVCS571tJHl9l3pkoR174xpT5/KyNjufk7ms2EeDOg9L/3pDNXS+fSdUd14blwsT0m71ZzGvaRx75zVxueuwlcTo91r39x6Ny+HoaFN957qsFGv4tfLHiH4HsycA6vvKfdJuEf/QbBbYAAEAAAAgc2ADBAAAAIDMgQ0QAAAAADIHbIBAOil2P7lO0d0f+dXnxeXSausYs/pxMdrJed5YOSdtg3KyB0F+StmJ5KTs2+yw5zwmFfZQ2/0EJSmPnWmXxPGbt8TlruesbVP/p7Jpk+DufSguFy++xtQd/PA5cTn89eNx+bwzj5h2GzvF9qCYE7ufwZK1NagqI7Ef7t4Rlzf/h31Wyt34W05Tes2Vcfnw1fJMDzxk75u2wSuO2cXjAlk0hXH5PGyzhns55V+g7e7KU3bRtQ9L/6xMvfLTphnln5M+RreI7d6R37vWtFv/N3cQEJyy1wzb7L2vtsn6mHzDRXG5MGXfn21DMkcj2+Xea1ssImvro+25woK9biva/Wjw1gAAAABA5sAGCAAAAACZI5MqMC7YAF25rRKvInxyb5OdzN49fKVx8Ld2xuWK0lwUPc/KvAp0psW0tUqXXJeAVllpMT0REbsm+9NTpsW5nsQ2r8JajG+2/a1T7v7VffuTr7WCCe1SomBabmz5C+vi8oHX2YfixJSoTwuBiN3Hpq2b9sQzEtdp+9dF1zJ8tr1wx4nWCMDWKtz+iU/E5Uv++rdVjfeu0q8xby05penSKqv2YdtHYVypuKvK1b1k/77OKVWJDoTovz6VVpzalap68EyrenvqX+T9c/abd1PWyWt1lndPtZpKT3q13c557zfvicvtb35+XPZVW7mKfs+qMZSW128hJEAAAAAAyBzYAAEAAAAgc6wsFZiv8lCy1WBAIguXL9lmT6uKmLVYFa8fm3/KC+ffrNorVQ2T3Ae31VUB859qaM4EF55njsuinaDCmJT96MxaFeW8J85472gJbi75exvPL79ZQh/s3Ws9po6yVtH5F2vcNxHRsZeeGZdXfzabKrDiuL0pUwOipug5IFGc+z5kVVZT6yVK9HSfPAP9h2wk6A0j0+ocUY9FeU9d0zrLpCXY+QFRe5U3yBxFXkaSwkRjb0gie0/1WvK9NaudMn/5qWRVZK6iIq6rxyby/g7XarRqh1y37YTtbwvUXob2Ybm/vheYvt/ag8slROInIiqOScOJ9bah9tzTOd6iZbYOIQECAAAAQObABggAAAAAmQMbIAAAAABkjhVmA+Qbn4gNT7RtY1ye2GTtEbT7oCtIpuOoYPsLtq2Vc+5/StqNjVEiaXY+nju+Oe1kBM0WcrEf9jK0R4GyLVA2GYEfjVnphdmaeFD/fUNSNz4Zl93EpGnHXZ2N6/w5j9TFc6K35qLNBh+tVRnqt3VTEibTsTcVJy5RmaoTe1jZzIi8rW7zdL+8XnLevLedkA86jomtQaXTvpImN4rdT5pNlz83WWf9RyRK8vCvSrTuqbV2veT3N3ZnJiKKitI2p+x+uGob5lVdYUzmtdxn1xyp+ctNyYNT7bD2JawMSbT79cZPP2jHR0DTcVRieIxtaTd12u5H21j5oQ/yG9Y37HvGOufG5Vx5eS1ESIAAAAAAkDmwAQIAAABA5lhZKrAoKRMm0fR6UaGUuz0XWu2JnROxbfezVg0zvVbEiqWXXxCX24/Y8MGFIyMypGODpo7P3CDj2NATl4uHPTVapSZK5meT1WSLzbHL7X0LdLLDFPdHox7zXGjDPY/F5fxmcSunAatUqj72pPRxwbly2WHvvgXqfmn3+6kpO6bdD8vB1qvUOf7YVZWnxgl7VHLHXRfLtVSy0BWPd7+S3G19yn1yY0307pTwBy4lJIHvFg+EVf8oSXv3/7FNKFoekpdfcdQ+4Oy02ktFcQ7te1YnONYhKgoTtr9cQrgJ/50QKbVX/x7JwppqagAoPyzvON5sVWAm4rf6vctP24W0/81nx2UdFb9jMDlEgiaACgwAAAAAoLXBBggAAAAAmWNlqcA8gnO3x+XJtcojpWLbafF5qVfK05da76CuoyL6LQ6LeNfl7T6yvFnUN5Uda02dVgu0HxSR7tBlA6bd4MW1cZQ+6nlSLCHuTKtG4seUZ5ZJqpjcR1RIVlUcf6kkFzXRnomot1+yrQ6eI9ftOtRr2k1ukPul1SltI1Zs3/E1UU3qOYlSbrfvaZQfkWfq4EtkHBvvTe5jpeGrk7WoPW8fF4PxRFHz5IvWTV3Q+HMioqrNoZp5cu2iAommJWzvto8+YtoN/vyO5D4qOmSwFMM2u8BzSiWm5yUs+slQVX8pEfJNEuO7H0xsByw8KpmbObImBEnerL6aum24ORVWoJKhhtycKUQrAgkQAAAAADIHNkAAAAAAyBzYAAEAAAAgc6xoG6BjL5SolqGyEfCj0hr7Fa3O9NShJiPuhmRDl47jKrv8iOdaqmxbTjxP9LRT66zy9Kw/rEVyPeomaCkZe+Pz43I4Zu1oigluzjOywavjXJkSGfjGo3LgudrqSNCr71VpoT1bkGLYOBRCsHGDOdazEqiotL4Bk3kGvOdBu8VHL5DQB/S/Gg5hRTLD3kvfIx2GIGdvnh+BVtqlHOsu/HYpdmdZRNv9aMKhIXPcoSJyh+1e5PtpWeC5kqyRXMXLNJ6Q5T0/adeijqzP1cYu8URExRHPSBM0hVNhAiqdXoTnaR3GQD73bS1NtHVtspUWdlvbFCVHomlJIAECAAAAQObABggAAAAAmWNFq8DyJZWsU7lf+27wxr1W3ZHQc9m2rrvJ153YIJXjG61/ro56G7ZJ/52HWzO13x0f+lhcPutbv9HUOb46wopSrcg1WL9O6ooSxdm12wjYLif3NGhP8Xmu6mSoSgXTZvsLzpOIpyXlruuLcE0kaK9Of8//vOP2uPxVsqEPVjL+OihI4F6K1P3JhXYt6US6uTmIzYPS8oo426pol3M/REUwpdRUKtSHrw7RdblJ9XL1woO4QCUnTpm+YFL05Jjl5gmHRxLrjJpRm3z4+scEc5AZkZ8Tk6G25u9YEpAAAQAAACBzYAMEAAAAgMyBDRAAAAAAMseKtgEyGaJTQnRrt3htW+BnKa50yX5R2zfM6Fsdh35qhQS36oqfUqBFuOyeX4nLxefsl4kKyiZKJx/23cW13Y9Xd+zVKvtwJVnjr20VdH8z3Km1S2aTWcY7jqkHwP+TIEHXTURUGJUPPvTV18blbXQnZYWZ91XPjfq4mja3zV1Lr2c/nIIfRgIo2Bh9mCrtcj691trJ6eddu61Twbft0UYgyu7Oa6dTa3BFp8+w75XcyGRcNuZhOc+4MFpmPteLSKrbehrm/dm47B/r9RuUYAMEAAAAANDSYAMEAAAAgMyxPFVgCSJdbrPu0RVJIG5c32e4cSrJaqBE637E6LDYuC7y7qKJfOyJ/rWrafsJqSv3WBH+kd+9loiIql/4CS0l6173aGJdrktuMG8Ud/YTV6837azqwt6PtlGZjPykcrv1VFt6jow7erIGJvH8Geep+eoYtA/HwCfuatxhhilfvysup97XhKjQtbrmHJzNWtWZ5qft+dUOqcyftVU+37uvqetklcKgRJqfWueFnlBrMGyXifbVHLlIjqNCSkhu/a7WanHfEzuC8/tpM9dbOAdNslaHaTXncgASIAAAAABkDmyAAAAAAJA5lqUKjFVEUVdVyfyuvMC0q6qEcMXRBO8UD63OMl5kZFUvRiXjq9S4cTsim5S1d+9UXJ44s920G9ucm9FXqxFNqEStT+6NiwMVqzscuXJTYh9aXejU/Z4RaTspkmlKMk49L763kvYQ0/PceXCKQDodByTp4ui2flvZpOg9LRKw6S7hTzR/3osyJKi9ZoNaq2lzYuq8OYkCHUldedFO2vdApCJDR8Xkl7AL8Hf5aTPH343EiNHJAaPNmq922S2F7wTdauBJAwAAAEDmwAYIAAAAAJkDGyAAAAAAZI7lYQPkRQDVdj+a4sFhc8xhp5yjtnq+27qJyNyp9Nmeq25emYeEOoO8rxM3medtH4UJOR4+ryMuDzwwZtr1fOEhIiLa7yaoVeG83EgzJ54bqwkF4OuSEyJjLzQmsrSa59y0Z7egD+boyr3iqIoxXORH4l7ISdS2Bm0tbBzXaqQ8pxzKE66zv9fqlD3PtIrc7NlGBjoDvCLyIkEH4yWpaxfrkOWWQXxF0+Sy0jZB+h1e6ba/1bABAgAAAABoMbABAgAAAEDmWBgVmFJZsZ8skWXP5apKdJqmTmgy6d3Tb7Pu1u2D6rJKs5GWKE5HmPVdbXUCVJMMzutD95/mWjrdr9QwnuplOShXklSR4dFj5jhX3igHM6K+NnktrbKaw93xo8tq13czX6XG4vxapff3gstmMsaopz2xzlcbN0PaWkrCVzvrEBW5dhlfND096/GsOFKSobp2if4ctfmhIuQmVzvl5Vfutje/bUR+RrTarLTa/rwUx1SkaTWMqGivG63qooYg+WnTpIZPSXFvbxrzTEnRfzY6qbWBBAgAAAAAmQMbIAAAAABkjrmrwHK+fihsWHbzYOCfu3iHOT5x+eq4PH6GUmV4GplAqbOaVUtpUfqMnHxK6hemRJP2E6BqtNeMTqg6ekGfadf9UHL/LUOCaN2VSraZicjs9WESZqZcag6qFX0t9qXnSe4JSMR4Sia2qCS4/rwkzWfK/CVFe55BgueJf93cRknGGyEqdLoX2NhkXG4btKqnYFIyQ0dF+akoDtmFGnZKXVCShRYcsS///Liol7WHWG7KvrjDblGV4S/05slvkOd+hopYq5lT3rPGuyvlWkleYGHbzLatDJ4vAAAAAGQObIAAAAAAkDmwAQIAAABA5pi7DVCTLonBKmvbQmske3TljFVxudxrjTIGL5KhTQ9YHXYgqmlj95OfNM0obBdFZduwNkSx7XSm8Kr225vRTsodg9Jf6LlxTm5Qrv6eIjVQ5jHaLmVigzUq6qZsoG1IjN46TU+dtm03didzGFCardF8GLStAHQ4COdHx24yzXuSDUGzpNoxKNdukM70uWI3Ujxuo87nRuWFGqjo3xR67/6ivLtdIfknhaflxZ3rUS/anF3Q091S16l+P8LhEdtfQebZVcqUdaINA3G5MJVim5dGQjb41FOU3WS1w4v+fcG5cTl85IkmB7F4QAIEAAAAgMyBDRAAAAAAMsecVWDu2ueZ46duksSerqjUGnkre+NK4wiS+XFPdKZURYEn3cyVG7u+B17Q1+7nRFQ7dqbI7bs898ywoNzlQ+m73OtFRlWnTffLeH0VWF4lPM150mKjPgj0tWjFosWiQcne+6SkpPNOWgRqI/ZFks1ToRMGz5Vm1V4uSaXp56VV6yrs66Csk5SoOFgzYNqNrRP1VTBlfZjDLjk2ru6TXrwRbULQIRPhvES5Okq0nv8ZSVNVctSJF54fl9u/cbdpB7WXZfTcHjlI0+Q36eqeqjZLOLEwZk868qI1cXkNVGAAAAAAAEsPNkAAAAAAyByzUoFxLke5jpqF/t53WVVGEInHQGVCeWGU7R5Lq68okrKfeFSL2HKVZFWUU85j5dWmGfV8dzgu56dExzR0rvU4y2uLeVXsPmi/Y3FE9FnT/Srhq5cb0ozPv8Na/Kis5/PTy1D10qTKKk21ZaI1pzhZna6qLDUCtS4H+JvgVOi1mgtTvE3SVIuu8ZqboapsMlK4rjt2mUQ0XvuTlHNWMEmJissXbzXHHUeTk//mVILm8ip5p7cdtO62lQEdGVw+LwzZiPDl1aJSy4/pqNDWay+YlvfspFLRdZ9lx15FlG/D+CZZmN2HPFODJl9r1qsz5b2tf8Z1dgPP5GN0u5TXUOuBtz0AAAAAMgc2QAAAAADIHNgAAQAAACBzzMoG6NyLx+mbt95BREQ//7iN8Pzwni1x2WgRA6tHDLtEN6ldzn11Y7VXPiiMWAOh4rCcp7sIrMqZDlwvLp+bv/pcXO540u77oj6JPKqzHk+vt26h45uUa6m6c77tinZ1j/Ke/VKC7YOvo61ed0Xt87vvpOWODlWQauOR1oe+b82aS80hu3zUkZQmHpyktEpubN6POJtwz/3Izadr8TbDXkwNY2r9MrSnWyQmNlp7m+6D8tLk0LupgbbRTLmnaXUKvda5Ktcq99r3u7YByk/LSZVN1siTYQNkqOhQKgftupwRsT2JuSwdbQ/k7SiqPSokTJfYikUTNur4UgEJEAAAAAAyBzZAAAAAAMgcs1KB7at00W8duIaIiJ4+biOKug4RW/K4iDQLo1a8qVU9puxFjM6PSmXbsJXLaRVTTnlx5qfseAMlPj346jPicsdxK+otdyuVWkeyDFD3l+hGTZ543r/DWgycov7Z/4qaqLryaIbE+Qv4VX0RsA5BYMIWpLnBL2Sk6mVERWXqDbzwDTasQePIv3MlV1UR5r351O+BKMA8afLbxDyhbdT6KUd5mbB82da5ggr1oeZSfz6DlPdblKAqy1Vsw2qn9K/nvLzKqqetgUL2yPthAToav9NqH6hi02YHzbXT6u3ugzb8wtCFMpfDr70kLvd+vjXiU0ACBAAAAIDMgQ0QAAAAADLHrFRgo2OddNsPaklQC1utFXcw2Nh7ptplZXGFMZUYc8qEijXttAgv8iSuJmq0Oq3aadtxVSqDksjzSn2e+FxJ7XQi08ATzTqV9E+LbWeICrUY2EsOp79L2Ka8LHK23Zte/QMiIvrkP47RSiLVG6HJqL9NR4VuNoqw1mx64v0MKSCbpjQgi7P9uPc3lH72UyJ7ny7+mtOqkuo2LytyxqlsVN5TfuBu7emV97xjdZJSfV4u+e/msE3qCiPeA6C7C7SJg41GXVKqLq1604lWiYg6dl4oY939cOKYVioHXneG/YDnoHJOSSpsPJNTPWqlXOpLNnk5vlM66f18k+NbYCABAgAAAEDmwAYIAAAAAJkDGyAAAAAAZI5Z2QC1HZigs99dc187/K5rTV35CvFB58OSHr3tuBflU0Vr1lEj/azpRheZ4taq3XDZS4CsbXh0RNFccgJkG9XSsz0KytJHWFTXjZzXTpVLVg/OKjK0tluodNmL/eveS4mIaKh0d8pglyG+zpkTqnxbhVxjJfQM9/ZmXdWNW6h2r05sBupwv4oe7NpNnckOP8/e6GkZrfWafurln47L19PO+R3EMsREcfYe6FxZ3k++DZC207Hz6vu3q6J+L4a2HSsv+2qP2Pm0HbH2pOObJFp1UJH+yl128CM7JPRxz27KHDe+/fvm+Eu3vCQu+79JZuJV1Qz7Wt1MPQ5hwbebbbzOq14YGT2ObbsOUKsBCRAAAAAAMgc2QAAAAADIHLNSgWk2fPgOcxy+7PK4vO96+by0xkYXLQ6ryKMTSh3kea5qcamfiE+L6XKqnYnUTDZRo3HJ9cTAiclL/Xb5xuW8N/ZQRQSoeK6behyh0h60D1lVWe+NjxAR0T63/F16rYrp9JVKpj9fNXa6apcmEztmmc+/8ONx+T/d8XumzkSC1kvfU5sY1WdaJIMmk6vqUBZvf/ZFqmZlhZGYC5U+eSH5yZnz46KvD71EwKF6d6WpyrSaI1KqEr9driJ9TK2Ra3U8aUP4B2VRbenxllb76m7KNO9bu8ccf6H44rhcbfdCGqj3mjYNqbbZe3rGt47G5fCxJ+PyiXdcY/vLN+4v8qLhREWp+9OzvhaX30+XUysACRAAAAAAMgc2QAAAAADIHNgAAQAAACBzzM4GiJm4rZaD15VKpir43s/i8vbvqVOuvMS0e/qXJJX05CalVy547uIVtTfL+T7RqqqkbIq8bPCOVdoN5cLeNuq5ratL56fVmHx7kATbk7Sw/H6Ki+5nxOUzODYSl6v79lMWSHNlbgXmw0ZppfPdsYvjclRIWSM6lEVk22m7gVT3drWWQrUe/TQbOkP9PQcl+/kZZO0kssj0KpmI4oS9cbkxsTEsr7L51bWdR35Cvxc9255piUGQq4oLOzW7lso2LokOWVJRru+B/cmhwZ3SbjVlj+3f/nVz3Bbq+51sIOW7vmu43DhGzIy0Nknd+7+FKozBHz31i3G5SPuSB7GItPjPEQAAAADA/IMNEAAAAAAyx+xUYM7NUH2d8pR7HjTHZ92TMJCtm81x1NMVl3MTVrcV9ou8OzgqaiRXLtt2R45Sq6ElhNXEViuLYErkp1FPclgAthETLFrPaDKOJ0ebteJYTzabJC2GBuyU/OhNEl3ZvcLWlXuV2mRSPvfnKVJ+7FrN5d/+JHH95PrkiTrjL1Jk/Bmk2i73qn3Im4fuNr95jDYB4DBZBRZ2ie+zWetFOw/VTjnWY4oGek07vR5NuATvGcpvnKQsc+7NPzXHez8grup+5GZ97+YSPkCbkPj963d45O0oonZlevL78rvdKhEMIAECAAAAQObABggAAAAAmWPOkaDnmzQvKN8Anfaq8xZkNGA+iYqyz/Y967SKIyzqMMKU2M5ow7wtvPFWSGkXGGcHOanUZkOZdhLwCfc8Fpc37bV3aPx68fqcXiU3vdKZrLLSniL+PEXKqUjX9T7rRU7/pnh7RWOI/qyZ3Cj3t/uwrXOFZHWhvt+Riursq1Da9h6PyxMXrFfn2znv3DeqOldqL89bLGxv7GHbedTO+cbfV160M4efOc79O/kNPfpz1qRE3+MZHl0K19ne8HM/YrRJXq761slwiYjO/4SsxWj3w8kXXiIgAQIAAABA5sAGCAAAAACZAxsgAAAAAGSOlrEBAiuX4gmJNlsY8TJJK/daTnB/JSIiHXm7pHzd/S28dqt3zTlb6ujPuWkvlEJTPWQLzstrI5q0rsidX71LyurzYM2A7WT9mrhY7W1sd0BElD8mdiPuwCG57vS0aafNGvT4XBXWIZv/TOaEouQn2kvkTe1qzsLBE3E5WLfWtKuqcCNtzzyb2L+eo/YHGn9ORNS1O7ELe93mmmWG6v4Dcbn/0wdMXf7MM+Jy1N8Tl0vrukw79+zBhn2vftyGoql2iO1Yx1NiA1bdayM8p5gbtQSQAAEAAAAgc2ADBAAAAIDMwa5JNQERETMfI2qRLGbZYatzbu2pm80OzOWSgflcOWAuVxbzPp+YyyWjqbmc1QYIAAAAAGAlABUYAAAAADIHNkAAAAAAyBwtsQFi5l9kZsfMO5ps/wwzr2nw+fgsrzur9in93MzMm+ajr+UMMw8w8+76v8PM/Jw6Lp7i3Jcy8zcS6v6BmS9MqHsXM3d6n/0BM7+FmW9MOg+cGsxntmHmsD7Xe5j5fmb+r8zcEr8ZWQbrcv5olYf5TUT0IyL6laUeyBy5mYgyvwFyzg0653Y653YS0ceI6EMnj51z5VOdn9LvbzjnZiSSYeaAiN5FM1N2vZKIvk1ENxLRslyYrQDmM/NM1ef6IiJ6BRG9moj+xG/EzIgnt4hgXc4fS74BYuZuInoBEf06qQ1Qfad6OzPfwsyPMvPnmG3WPGbuYOZvMfP/1aDf/8bM9zDzA8z8vpTr/y9m/hkz38bMa+uf7WTmn9TP/Sozr076nJlvIqJdRPS5+g68Y15uzAqGmV+i/mK5j5lPRubqbjTf9edgV708zszvZ+a7iOg9VNt4fo+Zv1ev7yWiIhGdS0SvJaK/rF/n7JR5vZ2ZP8zMdzDzQ8x81eLekeUN5nPl45w7SkS/SUT/mWvczMxfYuavU+1HsOE7l5m7mPnf6hKkh5j5jfXP/5yZH663/asl+2IrGKzLJnDOLek/InorEX2yXr6DiC6vl19KRCNEdCbVNmp3EtEL63XPENE2IvouEf2q6mu8/v8riejjVMspniOibxDRixtc2xHRW+rl9xLRR+vlB4joJfXy+4now6f4/HYi2rXU97KV/hHRnxLRuxPqvk5EL6iXu6kWkTxtvuP7W5+zX1Z9PUNEa9TxLxHR++vlzxDRTaoubf4+US+/mIgeWur712r/MJ/Z+3fyfep9NkTJOAKGAAAgAElEQVRE66km9T5ARP31zxu+c4no9Sfnot6uj4j6iegxEi/kVUv9XZfrP6zL0/u35BIgqqm/vlAvf6F+fJK7nXMHnHMREe2m2qbnJF8jok875/6xQZ+vrP+7j4h+RkQ7qLZT9YmI6Iv18j8T0QuZuY9qC/L79c8/S0QvTvq86W8JND8mor9m5t+j2j09GdU+bb5PEhLRl1P6fhURfdP/sIn5+zwRkXPuB0TUy8yrZvF9sg7mMztoKfx3nHMnc2QkvXMfJKKfY+YPMvOLnHMjRDRKRNNE9A/M/EtEZPOpgPkC6/IULOkGiJkHiOg6qi2EZ4jovxHRG0+K5IiopJqHZHOX/ZiIblBtTddE9AEnetFznHOfbGJICIq0ADDz7yhR7Cbn3J8T0W8QUQcR/YTF+D1tvk8y7ZxLS9F1FRHdPYdh+nOPZyEBzGc2YebtVJvHk8m/JnQ1NXjnOuceJ6IrqLYR+gAzv7f+Q3wV1X5gbySiby3et1i5YF3OnqWWAN1ERP/onNvqnNvmnNtMRHuJ6IVNnPteIhokor9rUHcrEb2Da/ZFxMxnMPO6Bu1y9TEQEb2ZiH5U/wtliJlfVP/8bUT0/aTP6+UxIpIMc8DgnPtb9WI8yMxnO+cedM59kIjupdpfi3MlvvfMfBERPaoWblx3ivkjIjppm/BCIhqptwcNwHxmD67ZR36MamYCjX60Gr5zueYdO+mc+2ci+isiurzeps859+9UM67duTjfYmWDdTl7ltp6/01E9OfeZ1+m2mbkizObz+BdRPQpZv4L59z/e/JD59y3mfkCIrqzLiAap5qt0VHv/AkiuoiZf0o1vegb65//GhF9jGtuf08T0dtP8fln6p9PEdE1zjmbOhf4vIuZX0a1vz4eppoo9Zo59vVxIvomMx8ion8j+9fkF4joE3UR8E2UPH9EtUV7BxH1EtE75jiWrIL5XJl0MPNuqiWKrxLRPxHRXzdqmPLOPYdqBrIREVWI6J1U+7H8GjO3U01y9H8v9BfJKFiXpwCpMMCKgZm/QzWj+EOzPO92qhkS3rsgAwNzAvMJQOuxktblUkuAAJg3nHOvWOoxgPkD8wlA67GS1iUkQAAAAADIHEttBA0AAAAAsOhgAwQAAACAzIENEAAAAAAyBzZAAAAAAMgcs/ICK3Kba6euhRpLIpzz9mltxbgYtgfyuWfPnQvVB8rY23nBo6OCHOcq0o7HvQjtS2AvPk0TVHalRtGuT4ulmsusM0ZDx51za+e731acTy4U5CC0QWVdFDXXR6DWfl5eV64056TX8wbW5spiIdYm5nJpaHYuZ7UBaqcuuppfPvdRzZFcp/cAnbU5Lo5eKKlE9OaFiKhtqBKXWW2Gwja7oZrcIC/qzsNyTvH2+007V63SYnOXu21B+l2qucw633W37FuIfltxPvPrN8XlaNgGfI0mJvzmDQm6e+Vg3UBcDJ/ce3qDmwewNlcWC7E2MZdLQ7NzCRUYAAAAADLHggRCDNaov9SODya2y3V2xuXhGy81dVFeJMscWckOK+m5UwLoyXWBaTe6LWjYjr0Ub4UJ6X9yvUiDJt58pW032Vhs3/MNKymKpqcbtgNgJZDrkbR3bsc2Uze0o1vqtHbaUxSxa1zHzaqZX7DBHA7sHo7L0f2PNNkJACDLQAIEAAAAgMyBDRAAAAAAMgc2QAAAAADIHIueDDVYvTouD/78jrg8vtkaCfQ9JfY2QSXZMEB7d3UMWuMe7d6uXdj1OURktoHVDjlwKdvDoXPFwKF6005T13/3sbgcPv5UcicALAPG33C1OS53y8LwbXa0fV1QUqEnvLUUiLOl8d6sttv3gLYj0n07a+5Hg5cpb9BLnh+XO49az83Ct1smEfXyhX2DLjW5kWdguUjkdl5ojqPdD8flYP26uBwePWbaEXJhtgRHf+fauLzxO0dMnf4NZR0KYx68siEBAgAAAEDmwAYIAAAAAJljQVRgaa7vEy84Ny4XpkTNlZ+yMm3j+t6klFK7zhNZVZdxnfe2fVpVptv54tFcVY7zU8njGLtoTVzu2rvf1LnK0kewBeBUlF4jISBKfXbB5KeT16ZRiaW4t1fbVKVSe/nq6aR164eyCNR5+j0wurVg2vX+3BVxufDdnxJYPlTU3BERjWyXjABR0b77x94qatDVD0vdmi/ZF3c0NjafQwQ5TzetVKJaFXn8hrNNs+HniU682rne1G36S1GBuWh+VZaQAAEAAAAgc2ADBAAAAIDMseBeYEFvrzmudqjEo8qIO/IkZyZSrC9VC7S4MzkXoRG762SogZcMVd8F7S3mS9sSLpvzjNG1qD533lmmLtzzWOJ4AVhK8mdtjcsjq2XRGZWXh8t560+JqNOiOmt1csoSTr5ukFzHaq3nvXzGOtL7wOYzTV11/4HZDySL+J5TrjnPr/L1u+Jyx9Mn4vLwFetMuyOvKcXlqCov02vOe9q027d3W1zu+3G7qTvvU9J/+PDj0l9TI80QvkefqVPengW7VXClkmqn+vC8APNbJW/noVfLepuRjeG49L/5M0+YOtN0nr0MIQECAAAAQObABggAAAAAmQMbIAAAAABkjgW3AeLODnOsIy3nlRt8tdM0o1KPtAvKVuec5Lbuu7fbDNTcsDiDFLsF3X9JAs9S2wnbYUG55E5usTZQbXtSrg3AAtBs9NTjL9oUl7XdT6XTPt9mPc4IBa3KCRnfZzRrMhs8NxkZw9gledfVYx98ibUBWv0VsRuJJsV4iNvabP/a/gEY8tu3xWXn2Y0MnS9u60cv3xCXq912Nvu/J/Y8Aw+Ox+XBe4ZMu3OukvmbPNOz7qnIcx6sXRuXw2NeJOiskxYJW9l2uVKK7Y3qQ7u6ExEdfpWy+9FT5K3L7V+VeV7MOYIECAAAAACZAxsgAAAAAGSOBVeBRRsGzLEWd1fbZf9V7bSiOKdG5kJPxTQpbYsjIuosjFVMu1xFydyqUuaKFedxJHUuJ2Mqr+sy7SY2iQutceEvmmYUqvHmp+fg4wvAPJKk9iq/6kpzrJOS6ojMJqkwEQUqmLmvslqs1JIzrmvUaDoJqx17Lkyu4zM3yoFKwOjKiN6u8UOblHZJdP9gv6ipeNzGINj4fal78i1iQ1AcsvPQ/6k7pL8NEhXYf4qDR/fJwRkXmLqhXaKK6XmmR8YEFVgqwaq+uMx9Ms/R6h7TbvB50q7SI/NXGPOyJ6ilo5Oa+1kbck9KCIrFTKcLCRAAAAAAMgc2QAAAAADIHAuvAtv9sDkeeEpEadFFEiW53Ntt2q37V4kG2axVuPZ2ISLiYrFxXdEmSDR1FVGj5R86Ydr1qfLoH18rp3heMvkpEfWVeu0e08YrBWDpmBrwExDrg5QTExKeEvnR15vrbj7gJI8zX1Wm1F6+12h1jbyD+HFVkeYps5LwE1k67WKr1KP9q0yzXFmUFlPb++Py8UutbcDUTklEuu0fxJMu+N7PEodUPXwkebzrJel0YcwqyKod8o4fvERcjHtWW7Vv27/fk9z/csaP8JzgqXXkRpuUtNLVeH2Mn+MpIFmOe9eLB9fUHatNs5598gyFKmEt+yG5g9OXxXBBPW9Naq0hAQIAAABA5sAGCAAAAACZAxsgAAAAAGSOBbcBKt1gda6FSdEdFpXL5PqfeK7pbaLPG7r5GlOndZM6smvg6f2CstZhq75TUgKHbaKnnF7l2e8MyYlnfVpcMN3omO1jfEIO5jl7bWZI0WHPqd1CoG0mluE857wh+zYxiaTY2FBCVOcZzZq0D/IjSDc6P7VdlPw8+N9/9CyJWt93B2WPJp9hp99vREQsoU5OXCjv7U3ft+9F+ssH5zy0RoQDYrMVdlj7pb7HxS4lKkrd5CZrhRlcd4Uc3HbLvI5vSUl5D4ZHjsbldf8yZeqisTG/+ax44m+vNsfFEXmpFMZlTL1P2+uGxwflIO2druvYvrBcZfbhKiABAgAAAEDmwAYIAAAAAJljwVVgnU8OmuPKBnEmnzpH3BiffaUdyrZ/kwSEa79/0NS5dhGzVleJi6MrJu/njPurJ2LTYvLiqBLTPWIjmVJJRGxHbtgSl7sOW9Fx2wlpN3yOTQa79tanZexpLp6gOZpUeUUvuswcH71c5mXD/56jvmMZqr3yG1USyjYvfMN0QpRkT2WsVWW5SnJdqht8iiu9Ji05aiIp54QqAkbO9+xNUY0DwagriCj3Qzne8MPk83RS2flIKDu9VtRZ+Qm7FnMT0n9ur4yvcvb5pt1zb1Cqs9tOe0jLjlSV1xxU/N1PWVVkpA7zJVlgk2dYVaTJuZCaoFXr1e2YgvPPkYNHTzXSGpAAAQAAACBzYAMEAAAAgMyBDRAAAAAAMseC2wAdfel6c6xtbAqTohNsP2b3YqNbRV9cvuQMU+eUXlFF5J5pL5BkZ5Ci69d9u7y138lPSIflXumwY9CzpTguLpgD41bXDbufJmnStod3XWyOpzaKTdjQufJ4+89GflI+GH6bhFlY9U93Nj1EnUJl7BfFnbb7S3c13ceiE+jFk9xM6+5nupwn+Lr7qGa+m/q8p8ZIcr/3/sTTdj7+0CsdC52wY4WSZCviuzM3c84smOqXPrpL3kvdPKNSLq3y7D87/RzzICZtXhLmb8OdNkTCscvFuke/A8rddmF2qzQWae7s+TNlL/DwH9p9wd4bPx6Xg42JXRggAQIAAABA5sAGCAAAAACZY8FVYKuetCqg6X7xQ9XZqNtPWHl0/x5RI4Vt1rUuPy4iMiOOD6x4Myo0J+7XbvBcUWK/GRlrpZOhCySr/YnzvSzKJJFRx860dWt7xR0798P7kgeVcfIbrOp04jIJOzC+SR7byY12YgvKq1NHHvVdnkPRsNLoWdKHzXOdTvVFl8blsS1KHD+LPhabcKNk656hlmoyo3oulA8i79FPciWfkzt7Cv7YXZIKzFPDaBW3iRRPRBxK2/zmM+Nydf+BuQ80CySpSjw1tnF99zPPz4GqslDYf13B1PU/LO/gga8fi8ttw3ZMweE2AnPANV7ofOf95ji4WMwLdDb4oOI9G5dJeILgxLipG7lcfguGzxGZzZotx027V732reroTxuP2wMSIAAAAABkDmyAAAAAAJA5FlwFNj1gRZO5qoi+dOTZ9gM2pOz0OokUWeq14lKXs1EkY3w1V5PRZs0pSmTOngi3OCZiv969kswtbLfeYoVR0besesqKCvO7n4zLyyLwrFYhePeDm7TcTyPXJV4CQ794SVweOdvuzbW3X3FUyh1HPVGqdk7Iy9jbRuzdjgpSt/GO5Ki0werVcXn8Jeeauv2vlPKF//MZGcOF55l24cOPJ/a/2BjVUapaeG796/O0B1azyUtT+04LEKsiV+eUeN3Z14/twx+TGu/YFZvicgdUYPOPVpulqcNSPJHalTrLHfC8iM+Scv+UvKtPWKdR6jwEz7+mmGPSae1tq9+5kbcuH/8N9ZuufleIiDZ8R11WPSrDSs1JRNR/b/MevCeBBAgAAAAAmQMbIAAAAABkDmyAAAAAAJA5Ft4GqM/usYoqmnKlU3SCvcPWDqM0IDpBX/efK7vEOk2SnUHaOZG6I74dRLVdvktnSXTTYcFeqNotispKp/3+nVvEtoD2PJY8kKXERFFV4/ey7zZr96OjNR+/rMfUDe9QkcFH5VptJ7w+1FzoOQrbvciu2rRA2YJMrrfz0DYkdYMXiSvs5CuvMe2igrTrOGL7OO93ROccFkVvnZueplYlV24y6q5aI34IAcMSmVDMsFFKcNv3I0EH08mL3z5jsA1ZNJqMBO1Hfe97ZCQu9zxg039XX3a5HOzYHhfDdjv/Z3z+qbj8UFOjyChN2vz49H3uJ3F56Nfk3Vr1oq6f8zl5yVQ67bakpGKT6PfxwP2nH1sDEiAAAAAAZA5sgAAAAACQORZcBebDoZZPK9fVKesGbxKR+pIupaJJc6dNqpvRXUIEXOdHudVqgXGt5rBqnbAo+0pfBD+5pTcut+1pPL4lR4s7XXPi6ZG3Pj8uH73S665NdAvFQVunk+AGSgsa2sgCZl60SoZ9LVxCROD8lJ310HpaxnTttw/N+o8ku1ZOvP7quHz4WhU+oWr7OOtrk3Lw41sS+1sMjl7VF5cD/97peVdrzL9XnBw1YEExz0CzCY299ZcLdTv26uT7j6rI3l1zdAEGKTSZDPXwf7k2Lpf6bd3Znz0Yl6devcvU5SelT/eIqLm2fPtS0y48crSp4WYeP1RBk2rLY++8puHn6//mjsRz/Njc5TfKb4uO1t5/qw0vMpeUupAAAQAAACBzYAMEAAAAgMyx8CqwtGizWuI+PmnqokJfw3ZEXuLDqLHYPu1aM5JA6mSoKqKsLz43gaWHJBxxLlzjNUwOQV1WUa1bJQ3f1I1XmePDVysvttVaZ+CdWFR6iFDaFU7Yxyp/TI79CKA6maZTai/f82iGuqbODI++FA+gpP6Ckpw0ts22e+IjoubSqjwioqLyCms7LvNc6reDGn6PerZfnTymxaDSoxJ+HvOiaCc8+4VJr93p57Gcd/QaNh5cs1CR6++vE+lC5bUAKBVKftsWU/XUO86Iy+U+WXOFcTthB28Qj9qBPdbzUj8DuVVK7Tu1LOLvJ5PyG2eY72c2IfmpzzP/w6q81v1Mzuv8yl1N9cF5b1uivrJOdh0e9+wp5gAkQAAAAADIHNgAAQAAACBzYAMEAAAAgMyx6G7wSTrM6MSwPQ42NWyX2p+v92zSXd7QpOrUVVPC45oxeee1iP3EeZdO0q237iYior2VH5m6jx5/cVy+69i2uHxivNO0K02JQY8bUb7Snro4bJObkAvtRGjX95yKhDAj+rC+j2rbrqM9E1ndvw1p4D0AKiJsVUUkD8r+A6AuNmX/XtDfRds2Re2tZWcQnCuRcE2k8zDZtkdH1PbtaCL1fHOKrUFaxHXbMKUq4VbOsONT49Xf0V9/2lbI+dGeVdtq1/KKBM1eBu1mo7TPCwnZ3HNFa/AXqQjpwfnnxOVH32ltKDsPKnu6NTKx295zd9NDKl54nhz0iw1Q+xNHTLu0IOctyVLZo6Vcd++fid1P13N23TRr92MI7POk13PX4fl9t0ICBAAAAIDMgQ0QAAAAADLH4qvAEojGxsxxmguzEa0r0VyaS3SzmD48aZtxvy+IeDc3C2lztb01ROtPPNFPN9zwJiIiGtnRZ+pOXKRE0GuVe/sqGwI4KEhdtVf0V1W2ou/ctFIVeTJnLd7U5aBk75PLNZ7MXMW7n/q5URPmq1IKY2pMSnvgqyijolLfeRGeo7zUBeo7+pGg+QtemIRF5tArNjTVTq85rVJKSzzKcwm/mjqIuZ2mx6jHFHhjj4JktbhW57lm3Y1bhFSVV5qZwHygowKra0VeUuBg/bq4vPdXpNz/gBc24jw5Pu+3m1d7GfT3zMmDHQ2eaNB4eRKsXWuOS5dKOIHinY+YumjShplpBq1W9Z+vI78rEbqrvTL/6/5wDiovnyj5GW0/kfLCSTE9SQISIAAAAABkDmyAAAAAAJA5WkYF5qO9dnxvFSPqmm/xrlapRZ4aRt+tNhEPto9YOXtY1F4yXvctIll3TOTqouFVtz1h6nq+2DjCJrfZ2NW5bZvjculMUaOVrUaNSj16vmxdrqrvt3welO09zU/KcTCtosOOWdEsq6S6PF1SZdvOafG89ujzPBC0qnOGB6N6VqLhESl7ov+lJlCechU1ATlPmhwW9Pdz6nPbzjVuViOpzn/u0/pIwCU/RqaPnP++0M30n3x+Jys14PN8q73SksOmXOvgL4vnV5vSRI2eZdtt//3kBMRNoyODdytVzhxUQacDBwEFvbUXYjQxZepcqF0XpVy6wWaTHt0mPzzGO9a71aEyrwi2Pc/UDXxy9vdUq73yG9abuvFr5D5e8Icymb5XnY7qnOo5rcj19STWdTwlv02+Moz1u7tJZzFIgAAAAACQObABAgAAAEDmwAYIAAAAAJljwW2A/CzeJjpvij2McYmOUhrOg347aRwzbI/yjW2PiiNWtzm5zjOa0NdqkUjQNDlN7r49tfKaAVOV375NDoye2oscPDwal4sHDsXltqKNSttdTnHRVS6qFEWNPycvQ7DK2u1HoeW8OtbneFFpqUPsmVxXe1wOO712Opq4FzlYZw/PTYuruR91utqlxvGtW2ixWf/NZ+PyvrdsTWzHSSElfHfxqHG5UduTzMi8rqcwSm6XxEwX9ub6MFW+aWFzl25NUuxy9Hp27XZtVvs6pIs772/uWk2+c4//ps0MrsMTjG2XST/73T9pqr8ZWcL1kHz7kkDeH9UuWdOL/fp1YUihsg9shsl19ntq254oxQs8P6Hs9rxwK8FF50vdnsdmNR4ioif+y3Zz3LFb+q8+szvxvGbtfgz9q8yhfm7CJ56efX8pQAIEAAAAgMyBDRAAAAAAMseCqMC06DOf4hEcVNLcVWfvJ2vPISuqTYvsOgdXeqdErIURGyHZbRCRq68iiPwEjC1AeNxze/ePE8i1i+qItLt4aOW03ClJVNlXRTV773U73b9/jlad6bqSp4abFJdUHlJzWfDGp8IdUJjiW6mjCHvXyh07lnzeInDgDY3VXv6zqY9DHR3bW1dGNZyi2tJVM6K0J9zK1LARaUtHq9S01tbTeSSp3mZca7mR5n7+akks3XHMfunOI/KsFnqs+7Efnb8ZdLTnkXO9umkZY7NqL43zIwSn6YPU/aj0yM/cYqvASls76fE/rrm1n/nvVt7Q9Y374rJ2Oa+kJOLNqWTN/m9rXt1fX/N06GVi5rBuz6nHTUR06L9KtOfKBvsbd85f7Y3L8x4MvtMLt9LsBXj28hxIgAAAAACQObABAgAAAEDmWBAVWNuoskYvWnGejsRamExRgaWJoxO8VdwMt47GiQ99MTsr0WpaO929axcxXTDoiYq5O7mP5Sxm9zARj1ss+jEQOl5xNC6Pf19UFOypFHTkc71OB+44bNode7F4vEWeGklH9jZqLn/NJXhtzVibZn2rsvenW6SiWGv1XeeglZ+XepUSpJqiclXkLt5hr/XQo02dd1qc9G5MU/NoUrzA1n30jqa6aDJ4bipP/7ZEe3YbbOTjs983HJfnpDZp9l4QGY/VqX6Z8/ZGbReQjd3D9N4XfY2IiD615QWm7umrrojLWv1cGDXNKK+CV+t2lW4756XVyclAq2KFQBOvvzou992xz7Q79DoJyz1+ltzvzsetWipcwKSyldV2lnIppjIGN/snGBIgAAAAAGQObIAAAAAAkDmwAQIAAABA5lj4bPAp6rv8RIpO13jBexGIlVtuml1Akp3BDLOcXOPKtADUOsMwH7Zuzs7YHq2kcLNgOdJRqMTlSSnODMmQsFbDJ/ea49Xr++Ly+Garr9cR3LX9n3bRJbJrNTW7vPa4V+UwJZxEx5DYAnQ/ZNdm6VqxX5rhcp/w/R/9f7rM8XlvT7z0/DEbexeipkNIPPGZy+xpFZmIgbtsCIiBTzSXQfzwu8RdOneRGLB0/7DXtAufvI8WDRWmJFxswx/FiT1F+vyOWhiC/R9ba+py7cr2tCDlwqh11s9PNbapDW1Qb9POj0avs8gfuSqnymfZdur9wBXpY+st1g7QPJ1zCCOTRqXLbkuCcnO2PXOJOg0JEAAAAAAyBzZAAAAAAMgcC68C81HSssJ4sshKJ0DTor1anYjEjCtviitoaiToBPzEpZVO2S9GbVLJKcnufDF7tPh3HGQcp3RMkdJypCUUTYN/LMkPe1LaGbyktTk/IngTOLXudeTcNGYokpQKbEaoDe2Or07ce/0nTbPraWdT154rpa2d9Pif7CIios6nrJ6jZ5/cg76nxD86f9T6TruxibgcHj8un5ftPKxeL+ftePsRU3fiB+LSHj72ZFweeevzTbsNvyDJdvfevTkub/nbu+2YaBFR73ue71DFc6R4zN778oAMLJhQvyfeeHVi01B5o+f8RON+JoQEgunke+PyShU3ptSIaUlI56D28hPbavVVtcN+j/YTjSeQ26xrviuVGrZLAxIgAAAAAGQObIAAAAAAkDmwAQIAAABA5lgQi5RSr3J/nUpulyuLbm+G96tSl7oZo5T+q53Je7hmbRrMdZX6MSjbDrTbcGmV2DB0eO53iS6+RBR5rosALDSdb1Ox9G+Wom/jZphvgw3PrTuaXhrDjMKk2ND4YQB0mgF9b665//WmXS89tSBjO8mG7hH6/675JhERPf86a3uxNS/3bXXQSUkcqo7H5Vsnt8flD3zZ2u+UnpYs4fdXB0zd9DvkIeg8X953bz77NtPu//zgurh8wccOxOWq917kgrz8mrXhmjMl6b9v7wJfq0nO/vygOX7kd1fF5bBX7lU4au3jdNZ3bffj2+9Ye1jv4jqmi/5h9OyGtGv9OZ8fktO97ubb9V2j09oQEQUJ7woO7AtsLqOABAgAAAAAmQMbIAAAAABkjgVRgQ1fIMKotfd6lQnRmX36nhTdmcvbfVowLeJCHXWZ/YjRWkw3hwjMfrbs/KSIJotHxM3UFw9yQvRaopnRqgFYaMIjkg1+y8cl1OvQq863DdUa6Tkwe5fSVOYQhiKVOYrdOw/J9xrbakME67XZs1/E7t03LKzKy2doT4G+emEtavBXae0pWtcILjjXHB+9dk1cnl4j9371c/ZtVRyX73lkl1UpfPj1n47L/zFyYVz++G0vN+0u+NBzcbm6b3/iGBdc7aXgabnW0cvFXXrTbY1aLw7hw4+b4/PeKWXedXFcPrrL/iyHHY3d4Nn74clVkteYNr0Ijfe4XUedh1XIjPsfSezvdHFR8vod3WZ/JLsOybF5QqPZZ3/3wc8xAAAAADIHNkAAAAAAyBwLogJ76o0fi8vP/+l/SmwXFUWg5e/Ecj/aTc2QJlif77yj+malCt9S1Hzl3kWNhwqAIRwSzw5fHasTKFY75Gn3HRd1FNemExDOs6dIKikeKvkh5RG3xScGSWAAAAasSURBVE/kKuf13yVqwxYJJJxK+MgT5njAO26Gbf9qj//mj3aoI3njnUM/Me1mn4Jy4akeELXcmgc2LuFImsPd+1BcnmE2ogjWikrUbVpj6soD4hVY7bLqzFKvLPZKlzznxXG7Pnr/pbkEuKe9nlOS/W6+1WZW4Mcl0rj+3Y3KFTpdIAECAAAAQObABggAAAAAmQMbIAAAAABkjgWxAbp+k2RLbn+11fXlyqLFKxyRTMTLQc/eLH1Piwt/qd9aUKy5H3tO0Br0/8xGpuUpcRF3E/IM+2vThS2+WlPsE/R3zPmZsCvaeA+2eiuF4q0pRjXLjPDYMTnQZbI/5v4PezstH9xP99jjpIYpdkTNgl9jAAAAAGQObIAAAAAAkDnYzULUy8zHiGjfwg0HNGCrc665cLCzAHO5ZGA+Vw6Yy5XFvM8n5nLJaGouZ7UBAgAAAABYCUAFBgAAAIDMgQ0QAAAAADJHy26AmDlk5t3M/BAzf4mZO0/R/jPMfFO9fDsz71qckYJmYOb3MPMeZn6gPq9Xz2PfL2Xmb8xXfyAdrM2Vy0Ks02bmHM/FwoD5TKdlN0BENOWc2+mcu5iIykSUnFRskWHm4NStwEmY+Roi+nkiutw5dykR/RwR7V/aUdVg5gWJhbXCwdpcgbTyOgWzB/N5alp5A6T5IRGdw8zbmDnOGsfM72bmP007kZnfxMwP1v9a/WD9s3cy81+oNjcz80fq5bcy89313fL/OflCZeZxZn4/M99FRNcswHdcyWwkouPOuRIRkXPuuHPuIDM/w8zvY+af1edoBxERM3cx86eY+R5mvo+ZX1f/fBsz/7De/mfMfK1/IWa+sn7O9pR+bq5LLr5ORN9evNuwIsHaXDkkrdP31tfQQ8z8ceZattn6X/kfrM/J48z8ovrnHcz8hbrU4YtE1HHyAsz898x8b10q8b6l+JIZAvN5Clp+A1T/C/0GInpwDuduIqIPEtF1RLSTiK5k5huJ6BYi+iXV9I1E9EVmvqBefoFzbifVguC+pd6mi4gecs5d7Zz70Vy/T0b5NhFtri+qv2Pml6i64865y4no74no3fXP3kNE/+Gcu5KIXkZEf8nMXUR0lIheUW//RiL6G32R+oboY0T0Oufc0yn9ENV+KH/NOXfdQnzhLIC1ueJIWqcfdc5dWZf4dVBNqnCSvHPuKiJ6FxH9Sf2zdxLRZF3q8D+J6ArV/j3OuV1EdCkRvYSZL13IL5RxMJ+noJU3QB3MvJuI7iWiZ4nok3Po40oiut05d8w5VyWizxHRi51zx4joaWZ+PjMPENH5RPRjIno51Sb3nvq1X05E2+t9hUT05dP6RhnFOTdOtfv6m0R0jGo/aDfXq79S//+nRLStXn4lEf1+fQ5up1ok9y1EVCCiTzDzg0T0JSK6UF3mAiL6OBH9gnPu2VP0Q0T0HefciXn7ktkCa3MFkrJOX8bMd9XX3XVEdJE6rdH6fTER/XO9zweI6AHV/peZ+WdEdF+9H72GwTyC+Tw1rWz/MFX/Sy+GmatkN22nSnHCKXVfJKJfJqJHieirzjlXFwV+1jn3Bw3aTzvnWjwJUutSv3e3E9Ht9YX3a/Wqk8mZQpLnkYno9c65x3QfdZXKESJ6HtWeg2lVfYhqz8NlRHTwFP1cTUQTp/2lsgvW5gqlwTr9Lar9db/LObe/vgb13DZav0QNUjgx81lUk/Je6ZwbYubP0PJKU7XswHym08oSoEYcIaJ1zDzAzG1kRXeNuItqYrk1dXuBNxHR9+t1XyGiG+uffbH+2W1EdBMzryMiYuZ+Zt46318iazDz+cx8rvpoJ6VHR72ViH5X6aYvq3/eR0SHnHMREb2NiLTB6zARvYaI/oyZX3qKfsD8g7W5zElYpyf/eDjOzN1EdFMTXf2A6upJZr6Yaj+4RES9VPvDY4SZ11NNfQoWCMznqWllCdAMnHMVZn4/1V6ee6n2F2Ja+0PM/AdE9D2q/cX57865r9Xrhpj5YSK60Dl3d/2zh5n5j4jo28ycI6IKEf0OIZT56dJNRB9h5lVEVCWiJ6kmlk36kfzvRPRhInqgvnl5pt7274joy8z8BqrNqZHiOOeOMPMvENE3mfkdKf2AeQZrc0WQtE6HqWbn9QwR3dNEP39PRJ9m5geIaDcRnZzD+5n5PiLaQ0RPU021CRYOzOcpQCoMAAAAAGSO5aYCAwAAAAA4bbABAgAAAEDmwAYIAAAAAJkDGyAAAAAAZA5sgAAAAACQObABAgAAAEDmwAYIAAAAAJkDGyAAAAAAZI7/HyitnmF5D7T2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(trainX[i])\n",
    "    plt.xlabel(labels[ytrain[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4TbJGeSOIU4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac06XZZTOIU6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Softwares\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQpLv3aOIU_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model using model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O59C_-IgOIVB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Softwares\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 18s 294us/step - loss: 1.1142 - acc: 0.6642 - val_loss: 0.7383 - val_acc: 0.7436\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.6427 - acc: 0.7799 - val_loss: 0.6079 - val_acc: 0.7808\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.5576 - acc: 0.8048 - val_loss: 0.5550 - val_acc: 0.8053\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.5167 - acc: 0.8169 - val_loss: 0.5273 - val_acc: 0.8141\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.4915 - acc: 0.8253 - val_loss: 0.5081 - val_acc: 0.8234\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.4727 - acc: 0.8330 - val_loss: 0.4932 - val_acc: 0.8278\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.4580 - acc: 0.8383 - val_loss: 0.4823 - val_acc: 0.8302\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.4461 - acc: 0.8423 - val_loss: 0.4733 - val_acc: 0.8331\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.4361 - acc: 0.8460 - val_loss: 0.4622 - val_acc: 0.8384\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 0.4270 - acc: 0.8485 - val_loss: 0.4562 - val_acc: 0.8432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe02feb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=10, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdzDtGwDOIVF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndfpdidOIVI"
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwk3T5LJOIVN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNLR8tcBOIVP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.0682 - acc: 0.6500 - val_loss: 0.7334 - val_acc: 0.7442\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.6510 - acc: 0.7776 - val_loss: 0.6257 - val_acc: 0.7781\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.5799 - acc: 0.7982 - val_loss: 0.5794 - val_acc: 0.7979\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.5426 - acc: 0.8101 - val_loss: 0.5511 - val_acc: 0.8066\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.5165 - acc: 0.8186 - val_loss: 0.5319 - val_acc: 0.8134\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.4978 - acc: 0.8245 - val_loss: 0.5170 - val_acc: 0.8181\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.4824 - acc: 0.8305 - val_loss: 0.5055 - val_acc: 0.8243\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.4693 - acc: 0.8345 - val_loss: 0.4945 - val_acc: 0.8281\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.4586 - acc: 0.8381 - val_loss: 0.4855 - val_acc: 0.8311\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.4492 - acc: 0.8420 - val_loss: 0.4780 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4409 - acc: 0.8449 - val_loss: 0.4715 - val_acc: 0.8356\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.4331 - acc: 0.8473 - val_loss: 0.4650 - val_acc: 0.8389\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4265 - acc: 0.8496 - val_loss: 0.4596 - val_acc: 0.8406\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.4200 - acc: 0.8522 - val_loss: 0.4553 - val_acc: 0.8424\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.4145 - acc: 0.8550 - val_loss: 0.4506 - val_acc: 0.8441\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.4088 - acc: 0.8563 - val_loss: 0.4467 - val_acc: 0.8467\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.4040 - acc: 0.8578 - val_loss: 0.4430 - val_acc: 0.8471\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.3994 - acc: 0.8597 - val_loss: 0.4400 - val_acc: 0.8484\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.3954 - acc: 0.8612 - val_loss: 0.4360 - val_acc: 0.8489\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3911 - acc: 0.8626 - val_loss: 0.4331 - val_acc: 0.8505\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.3872 - acc: 0.8632 - val_loss: 0.4296 - val_acc: 0.8508\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3830 - acc: 0.8650 - val_loss: 0.4278 - val_acc: 0.8518\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.3799 - acc: 0.8669 - val_loss: 0.4253 - val_acc: 0.8532\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3760 - acc: 0.8681 - val_loss: 0.4228 - val_acc: 0.8538\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.3731 - acc: 0.8694 - val_loss: 0.4197 - val_acc: 0.8546\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.3700 - acc: 0.8702 - val_loss: 0.4179 - val_acc: 0.8553\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3667 - acc: 0.8700 - val_loss: 0.4153 - val_acc: 0.8568\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3636 - acc: 0.8717 - val_loss: 0.4141 - val_acc: 0.8553\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3610 - acc: 0.8730 - val_loss: 0.4122 - val_acc: 0.8561\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3585 - acc: 0.8739 - val_loss: 0.4106 - val_acc: 0.8568\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3557 - acc: 0.8749 - val_loss: 0.4076 - val_acc: 0.8584\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3531 - acc: 0.8755 - val_loss: 0.4062 - val_acc: 0.8580\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3506 - acc: 0.8763 - val_loss: 0.4049 - val_acc: 0.8578\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3482 - acc: 0.8776 - val_loss: 0.4036 - val_acc: 0.8589\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3463 - acc: 0.8778 - val_loss: 0.4014 - val_acc: 0.8591\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3444 - acc: 0.8786 - val_loss: 0.4004 - val_acc: 0.8591\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.3413 - acc: 0.8798 - val_loss: 0.3987 - val_acc: 0.8616\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.3396 - acc: 0.8804 - val_loss: 0.3983 - val_acc: 0.8609\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.3377 - acc: 0.8809 - val_loss: 0.3963 - val_acc: 0.8603\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.3363 - acc: 0.8816 - val_loss: 0.3946 - val_acc: 0.8616\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.3337 - acc: 0.8828 - val_loss: 0.3936 - val_acc: 0.8612\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.3317 - acc: 0.8829 - val_loss: 0.3927 - val_acc: 0.8613\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.3301 - acc: 0.8841 - val_loss: 0.3910 - val_acc: 0.8622\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3282 - acc: 0.8848 - val_loss: 0.3898 - val_acc: 0.8625\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3264 - acc: 0.8853 - val_loss: 0.3880 - val_acc: 0.8628\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.3249 - acc: 0.8862 - val_loss: 0.3875 - val_acc: 0.8634\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3231 - acc: 0.8864 - val_loss: 0.3864 - val_acc: 0.8641\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3217 - acc: 0.8865 - val_loss: 0.3850 - val_acc: 0.8646\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3196 - acc: 0.8869 - val_loss: 0.3850 - val_acc: 0.8661\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3184 - acc: 0.8877 - val_loss: 0.3837 - val_acc: 0.8646\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3168 - acc: 0.8880 - val_loss: 0.3830 - val_acc: 0.8645\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.3150 - acc: 0.8890 - val_loss: 0.3820 - val_acc: 0.8652\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.3139 - acc: 0.8893 - val_loss: 0.3809 - val_acc: 0.8663\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.3126 - acc: 0.8899 - val_loss: 0.3799 - val_acc: 0.8665\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.3109 - acc: 0.8900 - val_loss: 0.3787 - val_acc: 0.8666\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3098 - acc: 0.8903 - val_loss: 0.3785 - val_acc: 0.8657\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.3081 - acc: 0.8919 - val_loss: 0.3775 - val_acc: 0.8660\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.3068 - acc: 0.8916 - val_loss: 0.3771 - val_acc: 0.8673\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.3056 - acc: 0.8922 - val_loss: 0.3763 - val_acc: 0.8672\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.3039 - acc: 0.8923 - val_loss: 0.3756 - val_acc: 0.8675\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.3029 - acc: 0.8930 - val_loss: 0.3745 - val_acc: 0.8670\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.3013 - acc: 0.8934 - val_loss: 0.3738 - val_acc: 0.8666\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.3001 - acc: 0.8944 - val_loss: 0.3729 - val_acc: 0.8681\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.2988 - acc: 0.8940 - val_loss: 0.3723 - val_acc: 0.8673\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.2978 - acc: 0.8951 - val_loss: 0.3717 - val_acc: 0.8678\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2966 - acc: 0.8955 - val_loss: 0.3714 - val_acc: 0.8682\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2951 - acc: 0.8958 - val_loss: 0.3708 - val_acc: 0.8685\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.2940 - acc: 0.8968 - val_loss: 0.3708 - val_acc: 0.8678\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.2929 - acc: 0.8963 - val_loss: 0.3697 - val_acc: 0.8697\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2920 - acc: 0.8968 - val_loss: 0.3688 - val_acc: 0.8690\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.2907 - acc: 0.8971 - val_loss: 0.3676 - val_acc: 0.8697\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2893 - acc: 0.8978 - val_loss: 0.3683 - val_acc: 0.8692\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.2887 - acc: 0.8980 - val_loss: 0.3668 - val_acc: 0.8704\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2871 - acc: 0.8987 - val_loss: 0.3664 - val_acc: 0.8706\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2857 - acc: 0.8988 - val_loss: 0.3660 - val_acc: 0.8708\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.2848 - acc: 0.8993 - val_loss: 0.3655 - val_acc: 0.8709\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.2832 - acc: 0.9001 - val_loss: 0.3658 - val_acc: 0.8698\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.2831 - acc: 0.9002 - val_loss: 0.3660 - val_acc: 0.8690\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2813 - acc: 0.9008 - val_loss: 0.3634 - val_acc: 0.8716\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.2804 - acc: 0.9008 - val_loss: 0.3633 - val_acc: 0.8712\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2793 - acc: 0.9013 - val_loss: 0.3634 - val_acc: 0.8698\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2784 - acc: 0.9015 - val_loss: 0.3631 - val_acc: 0.8711\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2772 - acc: 0.9012 - val_loss: 0.3629 - val_acc: 0.8713\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2760 - acc: 0.9020 - val_loss: 0.3618 - val_acc: 0.8731\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2753 - acc: 0.9026 - val_loss: 0.3611 - val_acc: 0.8718\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2743 - acc: 0.9033 - val_loss: 0.3615 - val_acc: 0.8716\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2733 - acc: 0.9038 - val_loss: 0.3600 - val_acc: 0.8723\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2724 - acc: 0.9040 - val_loss: 0.3608 - val_acc: 0.8708\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.2719 - acc: 0.9039 - val_loss: 0.3598 - val_acc: 0.8728\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2700 - acc: 0.9046 - val_loss: 0.3597 - val_acc: 0.8726\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2691 - acc: 0.9050 - val_loss: 0.3586 - val_acc: 0.8723\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2686 - acc: 0.9047 - val_loss: 0.3592 - val_acc: 0.8709\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.2672 - acc: 0.9064 - val_loss: 0.3587 - val_acc: 0.8732\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.2665 - acc: 0.9060 - val_loss: 0.3578 - val_acc: 0.8730\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2658 - acc: 0.9063 - val_loss: 0.3578 - val_acc: 0.8722\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2643 - acc: 0.9068 - val_loss: 0.3586 - val_acc: 0.8729\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2637 - acc: 0.9061 - val_loss: 0.3577 - val_acc: 0.8723\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.2627 - acc: 0.9071 - val_loss: 0.3567 - val_acc: 0.8739\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2615 - acc: 0.9081 - val_loss: 0.3580 - val_acc: 0.8715\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.2609 - acc: 0.9075 - val_loss: 0.3563 - val_acc: 0.8728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f508240>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=100, batch_size=600, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py-KwkmjOIVU"
   },
   "source": [
    "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLXUE9jWOIVV"
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "sgd = SGD(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJUqA5T4OIVc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 1.8881 - acc: 0.3699 - val_loss: 1.4306 - val_acc: 0.5350\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 1.2450 - acc: 0.6042 - val_loss: 1.1469 - val_acc: 0.6211\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 1.0474 - acc: 0.6589 - val_loss: 1.0112 - val_acc: 0.6583\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.9440 - acc: 0.6881 - val_loss: 0.9277 - val_acc: 0.6817\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.8782 - acc: 0.7084 - val_loss: 0.8720 - val_acc: 0.6987\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.8316 - acc: 0.7212 - val_loss: 0.8316 - val_acc: 0.7098\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.7962 - acc: 0.7329 - val_loss: 0.8005 - val_acc: 0.7219\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.7680 - acc: 0.7422 - val_loss: 0.7757 - val_acc: 0.7320\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.7447 - acc: 0.7503 - val_loss: 0.7549 - val_acc: 0.7390\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.7248 - acc: 0.7552 - val_loss: 0.7372 - val_acc: 0.7454\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.7075 - acc: 0.7607 - val_loss: 0.7215 - val_acc: 0.7515\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.6926 - acc: 0.7660 - val_loss: 0.7078 - val_acc: 0.7565\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.6793 - acc: 0.7704 - val_loss: 0.6957 - val_acc: 0.7617\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.6673 - acc: 0.7742 - val_loss: 0.6850 - val_acc: 0.7649\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.6567 - acc: 0.7766 - val_loss: 0.6752 - val_acc: 0.7697\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.6467 - acc: 0.7800 - val_loss: 0.6663 - val_acc: 0.7722\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.6378 - acc: 0.7823 - val_loss: 0.6582 - val_acc: 0.7757\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.6295 - acc: 0.7847 - val_loss: 0.6507 - val_acc: 0.7778\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6215 - acc: 0.7873 - val_loss: 0.6437 - val_acc: 0.7805\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.6145 - acc: 0.7901 - val_loss: 0.6373 - val_acc: 0.7826\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.6080 - acc: 0.7923 - val_loss: 0.6313 - val_acc: 0.7843\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.6015 - acc: 0.7941 - val_loss: 0.6257 - val_acc: 0.7860\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.5956 - acc: 0.7959 - val_loss: 0.6203 - val_acc: 0.7876\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.5903 - acc: 0.7977 - val_loss: 0.6152 - val_acc: 0.7888\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.5851 - acc: 0.8001 - val_loss: 0.6105 - val_acc: 0.7900\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.5799 - acc: 0.8014 - val_loss: 0.6060 - val_acc: 0.7914\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.5747 - acc: 0.8020 - val_loss: 0.6018 - val_acc: 0.7916\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.5711 - acc: 0.8033 - val_loss: 0.5977 - val_acc: 0.7930\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.5664 - acc: 0.8048 - val_loss: 0.5938 - val_acc: 0.7942\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.5622 - acc: 0.8060 - val_loss: 0.5901 - val_acc: 0.7957\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.5588 - acc: 0.8071 - val_loss: 0.5865 - val_acc: 0.7965\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.5549 - acc: 0.8093 - val_loss: 0.5831 - val_acc: 0.7976\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.5513 - acc: 0.8097 - val_loss: 0.5799 - val_acc: 0.7981\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.5480 - acc: 0.8111 - val_loss: 0.5768 - val_acc: 0.7996\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.5446 - acc: 0.8127 - val_loss: 0.5738 - val_acc: 0.8009\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.5416 - acc: 0.8129 - val_loss: 0.5710 - val_acc: 0.8011\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.5384 - acc: 0.8139 - val_loss: 0.5683 - val_acc: 0.8019\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.5360 - acc: 0.8149 - val_loss: 0.5656 - val_acc: 0.8028\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.5325 - acc: 0.8158 - val_loss: 0.5631 - val_acc: 0.8039\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.5301 - acc: 0.8166 - val_loss: 0.5606 - val_acc: 0.8039\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.5274 - acc: 0.8177 - val_loss: 0.5582 - val_acc: 0.8050\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.5254 - acc: 0.8189 - val_loss: 0.5559 - val_acc: 0.8065\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.5228 - acc: 0.8190 - val_loss: 0.5536 - val_acc: 0.8078\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.5201 - acc: 0.8198 - val_loss: 0.5515 - val_acc: 0.8085\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.5180 - acc: 0.8205 - val_loss: 0.5494 - val_acc: 0.8092\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.5155 - acc: 0.8217 - val_loss: 0.5473 - val_acc: 0.8102\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.5131 - acc: 0.8220 - val_loss: 0.5453 - val_acc: 0.8106\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.5112 - acc: 0.8231 - val_loss: 0.5434 - val_acc: 0.8105\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.5094 - acc: 0.8234 - val_loss: 0.5415 - val_acc: 0.8111\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.5069 - acc: 0.8242 - val_loss: 0.5397 - val_acc: 0.8122\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.5050 - acc: 0.8250 - val_loss: 0.5379 - val_acc: 0.8126\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.5034 - acc: 0.8250 - val_loss: 0.5361 - val_acc: 0.8131\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.5014 - acc: 0.8262 - val_loss: 0.5344 - val_acc: 0.8135\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.4993 - acc: 0.8268 - val_loss: 0.5327 - val_acc: 0.8136\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.4978 - acc: 0.8271 - val_loss: 0.5311 - val_acc: 0.8149\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.4962 - acc: 0.8278 - val_loss: 0.5294 - val_acc: 0.8155\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4940 - acc: 0.8282 - val_loss: 0.5279 - val_acc: 0.8162\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4922 - acc: 0.8289 - val_loss: 0.5264 - val_acc: 0.8166\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.4902 - acc: 0.8303 - val_loss: 0.5249 - val_acc: 0.8177\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.4893 - acc: 0.8298 - val_loss: 0.5234 - val_acc: 0.8181\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.4876 - acc: 0.8307 - val_loss: 0.5220 - val_acc: 0.8184\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.4859 - acc: 0.8311 - val_loss: 0.5206 - val_acc: 0.8190\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.4845 - acc: 0.8321 - val_loss: 0.5192 - val_acc: 0.8196\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.4828 - acc: 0.8315 - val_loss: 0.5179 - val_acc: 0.8199\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.4811 - acc: 0.8329 - val_loss: 0.5166 - val_acc: 0.8209\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.4804 - acc: 0.8328 - val_loss: 0.5154 - val_acc: 0.8210\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.4784 - acc: 0.8333 - val_loss: 0.5141 - val_acc: 0.8215\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.4773 - acc: 0.8336 - val_loss: 0.5129 - val_acc: 0.8218\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.4758 - acc: 0.8344 - val_loss: 0.5117 - val_acc: 0.8218\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.4746 - acc: 0.8347 - val_loss: 0.5106 - val_acc: 0.8220\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4732 - acc: 0.8351 - val_loss: 0.5093 - val_acc: 0.8223\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.4717 - acc: 0.8357 - val_loss: 0.5082 - val_acc: 0.8221\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.4704 - acc: 0.8364 - val_loss: 0.5071 - val_acc: 0.8223\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4693 - acc: 0.8362 - val_loss: 0.5060 - val_acc: 0.8229\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.4685 - acc: 0.8369 - val_loss: 0.5049 - val_acc: 0.8230\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4669 - acc: 0.8378 - val_loss: 0.5038 - val_acc: 0.8239\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.4659 - acc: 0.8378 - val_loss: 0.5027 - val_acc: 0.8234\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.4644 - acc: 0.8388 - val_loss: 0.5017 - val_acc: 0.8240\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.4634 - acc: 0.8381 - val_loss: 0.5007 - val_acc: 0.8242\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.4623 - acc: 0.8393 - val_loss: 0.4997 - val_acc: 0.8250\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.4611 - acc: 0.8390 - val_loss: 0.4988 - val_acc: 0.8254\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4600 - acc: 0.8397 - val_loss: 0.4978 - val_acc: 0.8257\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4590 - acc: 0.8397 - val_loss: 0.4968 - val_acc: 0.8266\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.4583 - acc: 0.8404 - val_loss: 0.4959 - val_acc: 0.8265\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.4570 - acc: 0.8409 - val_loss: 0.4950 - val_acc: 0.8262\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.4559 - acc: 0.8413 - val_loss: 0.4941 - val_acc: 0.8268\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.4549 - acc: 0.8417 - val_loss: 0.4932 - val_acc: 0.8271\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.4536 - acc: 0.8411 - val_loss: 0.4923 - val_acc: 0.8281\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.4528 - acc: 0.8419 - val_loss: 0.4914 - val_acc: 0.8284\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.4520 - acc: 0.8418 - val_loss: 0.4906 - val_acc: 0.8281\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.4509 - acc: 0.8430 - val_loss: 0.4897 - val_acc: 0.8279\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.4500 - acc: 0.8424 - val_loss: 0.4889 - val_acc: 0.8288\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.4491 - acc: 0.8433 - val_loss: 0.4881 - val_acc: 0.8285\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.4483 - acc: 0.8433 - val_loss: 0.4874 - val_acc: 0.8289\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.4473 - acc: 0.8438 - val_loss: 0.4866 - val_acc: 0.8288\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.4463 - acc: 0.8440 - val_loss: 0.4858 - val_acc: 0.8287\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.4454 - acc: 0.8453 - val_loss: 0.4849 - val_acc: 0.8294\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.4444 - acc: 0.8447 - val_loss: 0.4842 - val_acc: 0.8299\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.4436 - acc: 0.8454 - val_loss: 0.4835 - val_acc: 0.8298\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.4432 - acc: 0.8451 - val_loss: 0.4827 - val_acc: 0.8306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x55bacc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=100, batch_size=600, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9CSqKvpOIVk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGAad54JOIVm"
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "sgd = SGD(lr=0.03)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQ7oIymROIVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.7455 - acc: 0.7503 - val_loss: 0.6073 - val_acc: 0.7897\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.5239 - acc: 0.8186 - val_loss: 0.5504 - val_acc: 0.8078\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.4842 - acc: 0.8318 - val_loss: 0.5158 - val_acc: 0.8216\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.4626 - acc: 0.8388 - val_loss: 0.4935 - val_acc: 0.8309\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4482 - acc: 0.8432 - val_loss: 0.4842 - val_acc: 0.8327\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4367 - acc: 0.8470 - val_loss: 0.4749 - val_acc: 0.8366\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.4278 - acc: 0.8495 - val_loss: 0.4654 - val_acc: 0.8404\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.4201 - acc: 0.8528 - val_loss: 0.4603 - val_acc: 0.8414\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.4138 - acc: 0.8546 - val_loss: 0.4567 - val_acc: 0.8422\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.4094 - acc: 0.8563 - val_loss: 0.4536 - val_acc: 0.8446\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4041 - acc: 0.8587 - val_loss: 0.4486 - val_acc: 0.8458\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3999 - acc: 0.8593 - val_loss: 0.4475 - val_acc: 0.8460\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3961 - acc: 0.8604 - val_loss: 0.4454 - val_acc: 0.8451\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3926 - acc: 0.8617 - val_loss: 0.4445 - val_acc: 0.8456\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3887 - acc: 0.8632 - val_loss: 0.4409 - val_acc: 0.8466\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.3864 - acc: 0.8633 - val_loss: 0.4389 - val_acc: 0.8480\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3833 - acc: 0.8648 - val_loss: 0.4375 - val_acc: 0.8488\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.3801 - acc: 0.8659 - val_loss: 0.4342 - val_acc: 0.8498\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.3778 - acc: 0.8658 - val_loss: 0.4320 - val_acc: 0.8512\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.3755 - acc: 0.8676 - val_loss: 0.4358 - val_acc: 0.8477\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.3735 - acc: 0.8683 - val_loss: 0.4316 - val_acc: 0.8518\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.3704 - acc: 0.8686 - val_loss: 0.4314 - val_acc: 0.8512\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.3689 - acc: 0.8692 - val_loss: 0.4293 - val_acc: 0.8498\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3667 - acc: 0.8705 - val_loss: 0.4287 - val_acc: 0.8512\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3644 - acc: 0.8715 - val_loss: 0.4243 - val_acc: 0.8523\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3628 - acc: 0.8715 - val_loss: 0.4256 - val_acc: 0.8524\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.3609 - acc: 0.8727 - val_loss: 0.4333 - val_acc: 0.8492\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3585 - acc: 0.8733 - val_loss: 0.4237 - val_acc: 0.8525\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.3572 - acc: 0.8740 - val_loss: 0.4203 - val_acc: 0.8547\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3551 - acc: 0.8744 - val_loss: 0.4233 - val_acc: 0.8526\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.3530 - acc: 0.8750 - val_loss: 0.4224 - val_acc: 0.8545\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3514 - acc: 0.8752 - val_loss: 0.4216 - val_acc: 0.8526\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3498 - acc: 0.8756 - val_loss: 0.4181 - val_acc: 0.8551\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3484 - acc: 0.8763 - val_loss: 0.4184 - val_acc: 0.8527\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3461 - acc: 0.8777 - val_loss: 0.4166 - val_acc: 0.8550\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.3445 - acc: 0.8784 - val_loss: 0.4141 - val_acc: 0.8562\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.3434 - acc: 0.8777 - val_loss: 0.4140 - val_acc: 0.8569\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.3411 - acc: 0.8780 - val_loss: 0.4122 - val_acc: 0.8569\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3392 - acc: 0.8789 - val_loss: 0.4144 - val_acc: 0.8562\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.3377 - acc: 0.8792 - val_loss: 0.4115 - val_acc: 0.8549\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.3358 - acc: 0.8808 - val_loss: 0.4139 - val_acc: 0.8581\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3347 - acc: 0.8811 - val_loss: 0.4066 - val_acc: 0.8591\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.3331 - acc: 0.8812 - val_loss: 0.4128 - val_acc: 0.8579\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.3309 - acc: 0.8819 - val_loss: 0.4073 - val_acc: 0.8584\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3294 - acc: 0.8828 - val_loss: 0.4042 - val_acc: 0.8597\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.3288 - acc: 0.8836 - val_loss: 0.4089 - val_acc: 0.8585\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.3270 - acc: 0.8835 - val_loss: 0.4031 - val_acc: 0.8603\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.3261 - acc: 0.8842 - val_loss: 0.4014 - val_acc: 0.8605\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.3238 - acc: 0.8845 - val_loss: 0.4043 - val_acc: 0.8588\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.3222 - acc: 0.8853 - val_loss: 0.4022 - val_acc: 0.8614\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.3208 - acc: 0.8853 - val_loss: 0.4011 - val_acc: 0.8595\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.3190 - acc: 0.8862 - val_loss: 0.3989 - val_acc: 0.8612\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.3174 - acc: 0.8863 - val_loss: 0.3966 - val_acc: 0.8616\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.3161 - acc: 0.8875 - val_loss: 0.4062 - val_acc: 0.8581\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.3147 - acc: 0.8878 - val_loss: 0.3983 - val_acc: 0.8613\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.3136 - acc: 0.8884 - val_loss: 0.3950 - val_acc: 0.8625\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.3124 - acc: 0.8882 - val_loss: 0.3932 - val_acc: 0.8627\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.3106 - acc: 0.8893 - val_loss: 0.3951 - val_acc: 0.8617\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 19s 324us/step - loss: 0.3089 - acc: 0.8899 - val_loss: 0.3928 - val_acc: 0.8635\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.3078 - acc: 0.8907 - val_loss: 0.3942 - val_acc: 0.8631\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.3065 - acc: 0.8907 - val_loss: 0.3929 - val_acc: 0.8620\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.3049 - acc: 0.8911 - val_loss: 0.3929 - val_acc: 0.8611\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.3037 - acc: 0.8915 - val_loss: 0.3904 - val_acc: 0.8627\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.3025 - acc: 0.8923 - val_loss: 0.3900 - val_acc: 0.8635\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3008 - acc: 0.8932 - val_loss: 0.3891 - val_acc: 0.8631\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2996 - acc: 0.8936 - val_loss: 0.3880 - val_acc: 0.8648\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2984 - acc: 0.8934 - val_loss: 0.3894 - val_acc: 0.8600\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2965 - acc: 0.8949 - val_loss: 0.3873 - val_acc: 0.8640\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.2950 - acc: 0.8957 - val_loss: 0.3856 - val_acc: 0.8622\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2942 - acc: 0.8951 - val_loss: 0.3881 - val_acc: 0.8642\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.2935 - acc: 0.8960 - val_loss: 0.3861 - val_acc: 0.8632\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.2918 - acc: 0.8953 - val_loss: 0.3883 - val_acc: 0.8628\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2915 - acc: 0.8966 - val_loss: 0.3871 - val_acc: 0.8613\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2901 - acc: 0.8968 - val_loss: 0.3875 - val_acc: 0.8637\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2887 - acc: 0.8971 - val_loss: 0.3851 - val_acc: 0.8624\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.2872 - acc: 0.8970 - val_loss: 0.3859 - val_acc: 0.8653\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2862 - acc: 0.8976 - val_loss: 0.3813 - val_acc: 0.8642\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2845 - acc: 0.8986 - val_loss: 0.3840 - val_acc: 0.8625\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.2843 - acc: 0.8987 - val_loss: 0.3801 - val_acc: 0.8648\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.2822 - acc: 0.8996 - val_loss: 0.3836 - val_acc: 0.8653\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2818 - acc: 0.8993 - val_loss: 0.3805 - val_acc: 0.8658\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2805 - acc: 0.9009 - val_loss: 0.3796 - val_acc: 0.8642\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2791 - acc: 0.9001 - val_loss: 0.3786 - val_acc: 0.8664\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.2793 - acc: 0.9004 - val_loss: 0.3781 - val_acc: 0.8658\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2774 - acc: 0.9009 - val_loss: 0.3788 - val_acc: 0.8656\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2762 - acc: 0.9015 - val_loss: 0.3804 - val_acc: 0.8657\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2749 - acc: 0.9024 - val_loss: 0.3767 - val_acc: 0.8663\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.2742 - acc: 0.9028 - val_loss: 0.3844 - val_acc: 0.8639\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2729 - acc: 0.9026 - val_loss: 0.3816 - val_acc: 0.8655\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.2717 - acc: 0.9025 - val_loss: 0.3786 - val_acc: 0.8667\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.2708 - acc: 0.9038 - val_loss: 0.3812 - val_acc: 0.8662\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.2700 - acc: 0.9027 - val_loss: 0.3758 - val_acc: 0.8679\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2692 - acc: 0.9038 - val_loss: 0.3746 - val_acc: 0.8677\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2677 - acc: 0.9045 - val_loss: 0.3773 - val_acc: 0.8652\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2672 - acc: 0.9049 - val_loss: 0.3744 - val_acc: 0.8672\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2661 - acc: 0.9047 - val_loss: 0.3742 - val_acc: 0.8683\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2647 - acc: 0.9062 - val_loss: 0.3731 - val_acc: 0.8688\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.2644 - acc: 0.9058 - val_loss: 0.3736 - val_acc: 0.8696\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2638 - acc: 0.9060 - val_loss: 0.3750 - val_acc: 0.8662\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.2627 - acc: 0.9058 - val_loss: 0.3725 - val_acc: 0.8682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3615aef0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=100, batch_size=600, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr2YsZV0OIV0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 141us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.82"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(testX, testY)\n",
    "scores[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 125us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.88000000000001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(trainX, trainY)\n",
    "scores[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIkbMEN5OIV7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "R6_ExternalLab_AIML.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
